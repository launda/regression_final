{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Course 2: Machine Learning: Regression\n",
    "## Regression Week 2: Multiple Regression (gradient descent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first notebook we explored multiple regression using graphlab create. Now we will use graphlab along with numpy to solve for the regression weights with gradient descent.\n",
    "\n",
    "In this notebook we will cover estimating multiple regression weights via gradient descent. You will:\n",
    "* Add a constant column of 1's to a graphlab SFrame to account for the intercept\n",
    "* Convert an SFrame into a Numpy array\n",
    "* Write a predict_output() function using Numpy\n",
    "* Write a numpy function to compute the derivative of the regression weights with respect to a single feature\n",
    "* Write gradient descent function to compute the regression weights given an initial weight vector, step size and tolerance.\n",
    "* Use the gradient descent function to estimate regression weights for multiple features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.  Fire up graphlab create"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you have the latest version of graphlab (>= 1.7). Start GraphLab Create by importing graphlab, or if u just want to use sframe import sframe.\n",
    "\n",
    "There is a nice graphlab into [here](http://www.analyticsvidhya.com/blog/2015/12/started-graphlab-python/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#import graphlab as gl\n",
    "import sframe as sf  #pip install sframe while notebook was running and it found sframe!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load in house sales data\n",
    "\n",
    "Dataset is from house sales in King County, the region where the city of Seattle, WA is located.\n",
    "\n",
    "**IMPORTANT**: use the following types for columns when importing the csv files. Otherwise, they may not be imported correctly: [str, str, float, float, float, float, int, str, int, int, int, int, int, int, int, int, str, float, float, float, float]. If your tool of choice requires a dictionary of types for importing csv files (e.g. Pandas), use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtype_dict = {'bathrooms':float, 'waterfront':int, 'sqft_above':int, 'sqft_living15':float, 'grade':int, 'yr_renovated':int, 'price':float, 'bedrooms':float, 'zipcode':str, 'long':float, 'sqft_lot15':float, 'sqft_living':float, 'floors':str, 'condition':int, 'lat':float, 'date':str, 'sqft_basement':int, 'yr_built':int, 'id':str, 'sqft_lot':int, 'view':int}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] sframe.cython.cy_server: SFrame v1.10 started. Logging /tmp/sframe_server_1469358822.log\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /mnt/win-data/shared/stats-R/ml_specialization/machine-learning-specialization/course-2/Week2/kc_house_data.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /mnt/win-data/shared/stats-R/ml_specialization/machine-learning-specialization/course-2/Week2/kc_house_data.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 21613 lines in 0.056378 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 21613 lines in 0.056378 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# sales = gl.SFrame('kc_house_data.gl/')\n",
    "\n",
    "# This is how to do if not using GraphLab Create\n",
    "\n",
    "sales = sf.SFrame.read_csv('kc_house_data.csv', column_type_hints = dtype_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to do any \"feature engineering\" like creating new features or adjusting existing ones we should do this directly using the SFrames as seen in the other Week 2 notebook. For this notebook, however, we will work with the existing features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Convert to Numpy Array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although SFrames offer a number of benefits to users (especially when using Big Data and built-in graphlab functions) in order to understand the details of the implementation of algorithms it's important to work with *a library that allows for direct (and optimized) matrix operations*. **Numpy** is a Python solution to work with matrices (or any multi-dimensional \"array\").\n",
    "\n",
    "Recall that the predicted value given the weights and the features is just the dot product between the feature and weight vector. Similarly, if we put all of the features row-by-row in a matrix then the predicted value for *all* the observations can be computed by **right multiplying** the \"feature matrix\" by the \"weight vector\". \n",
    "\n",
    "First we need to take the SFrame of our data and convert it into a 2D numpy array (also called a matrix). To do this we use graphlab's built in .to_dataframe() which converts the SFrame into a Pandas (another python library) dataframe. We can then use Panda's .as_matrix() to convert the dataframe into a numpy matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np # note this allows us to refer to numpy as np instead "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will write a function that will accept an SFrame, a list of feature names (e.g. ['sqft_living', 'bedrooms']) and a target feature or response/outcome e.g. ('price') and will return two things:\n",
    "* A numpy matrix whose columns are the desired features plus a constant column (this is how we create an 'intercept')\n",
    "* A numpy array containing the values of the output/response\n",
    "\n",
    "\n",
    "**3.** Next write a function that takes a data set, a list of features (e.g. [‘sqft_living’, ‘bedrooms’]), to be used as inputs, and a name of the output (e.g. ‘price’). This function should return a features_matrix (2D array) consisting of first a column of ones followed by columns containing the values of the input features in the data set in the same order as the input list. It should also return an output_array which is an array of the values of the output in the data set (e.g. ‘price’). e.g. if you’re using SFrames and numpy you can complete the following function:\n",
    "\n",
    "**Please note you will need GraphLab Create version at least 1.7.1 in order for .to_numpy() to work!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_numpy_data(data_sframe, features, output):\n",
    "    #   this is how we call this function\n",
    "    #   (example_features, example_output) = \n",
    "    #   get_numpy_data(sales, ['sqft_living'], 'price')\n",
    "    #   sales is the SFrame, features_list = ['sqft_living', ....], response = 'price')\n",
    "    \n",
    "    data_sframe['constant'] = 1 # this is how you add a new column to an SFrame, tis case 'constant'\n",
    "    \n",
    "    # add the column 'constant' to the front of the features list \n",
    "    # so that we can extract it along with the others:\n",
    "    features = ['constant'] + features # this is how you combine two lists\n",
    "    # features = 'constant' + features # this wont work!! - string constant add to a list\n",
    "    \n",
    "    # print \"print features\", features, \"\\n\"   prints nothing!!!\n",
    "    \n",
    "    # select the columns of data_SFrame given by the features list \n",
    "    # into the SFrame features_sframe (now including constant):\n",
    "    features_sframe = data_sframe[features]\n",
    "    # features_sframe = gl.SFrame.to_dataframe(data_sframe[features])  #if use gl.SFrame\n",
    "    # features_sframe = sf.SFrame.to_dataframe(data_sframe[features])\n",
    "    \n",
    "    print \"get_numpy_data(): print data_sframe = features_sframe(head 5)\\n\",features_sframe.head(5), \"\\n\"  \n",
    "    #prints the 2 columns constant and sqft_living\n",
    "    \n",
    "    # the following line will convert the features_SFrame into a numpy matrix:\n",
    "    feature_matrix = features_sframe.to_numpy()\n",
    "    len(feature_matrix)\n",
    "    print \"get_numpy_data(): print data_sframe converted to feature_matrix[0:5, :]\\n\",feature_matrix[0:5, :] , \"\\n\" \n",
    "    \n",
    "    # assign the column of data_sframe associated with the output to the SArray output_sarray\n",
    "    output_sarray = data_sframe[output]\n",
    "    \n",
    "    # the following will convert the SArray into a numpy array by first converting it to a list\n",
    "    output_array = output_sarray.to_numpy()\n",
    "    \n",
    "    return(feature_matrix, output_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For testing let's use the 'sqft_living' feature and a constant as our features and price as our output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_numpy_data(): print data_sframe = features_sframe(head 5)\n",
      "+----------+-------------+\n",
      "| constant | sqft_living |\n",
      "+----------+-------------+\n",
      "|    1     |    1180.0   |\n",
      "|    1     |    2570.0   |\n",
      "|    1     |    770.0    |\n",
      "|    1     |    1960.0   |\n",
      "|    1     |    1680.0   |\n",
      "+----------+-------------+\n",
      "[5 rows x 2 columns]\n",
      " \n",
      "\n",
      "get_numpy_data(): print data_sframe converted to feature_matrix[0:5, :]\n",
      "[[  1.00000000e+00   1.18000000e+03]\n",
      " [  1.00000000e+00   2.57000000e+03]\n",
      " [  1.00000000e+00   7.70000000e+02]\n",
      " [  1.00000000e+00   1.96000000e+03]\n",
      " [  1.00000000e+00   1.68000000e+03]] \n",
      "\n",
      "First row of the feature matrix --> example_features[0, :]\n",
      "[  1.00000000e+00   1.18000000e+03]\n",
      "\n",
      "First item in array/list --> example_output[0]\n",
      "221900.0\n"
     ]
    }
   ],
   "source": [
    "(example_features, example_output) = get_numpy_data(sales, ['sqft_living'], 'price') # the [] around 'sqft_living' makes it a list\n",
    "print \"First row of the feature matrix --> example_features[0, :]\\n\", example_features[0, :]  \n",
    "# this accesses the first row of the data matrix the ':' indicates 'all columns', in tis case only 2\n",
    "print \"\\nFirst item in array/list --> example_output[0]\\n\",example_output[0] # and the corresponding output/response series i.e price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Predicting output given regression weights\n",
    "If the features matrix (including a column of 1s for the constant) is stored as a 2D array (or matrix) and the regression weights are stored as a 1D array then the predicted output is just the dot product between the features matrix and the weights (with the weights on the right). Write a function ‘predict_output’ which accepts a 2D array ‘feature_matrix’ and a 1D array ‘weights’ and returns a 1D array ‘predictions’."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we had the weights [1.0, 1.0] and the features [1.0, 1180.0] and we wanted to compute the predicted output 1.0\\*1.0 + 1.0\\*1180.0 = 1181.0 this is the dot product between these two arrays. Note the weights are just the model coefficients, Bo, B1, B2 etc or w0,w1,w2 etc. If both features matrix and weights array are numpy arrays we can use np.dot() to compute this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Coefficients or weights [ 1.  1.]\n",
      "features matrix [  1.00000000e+00   1.18000000e+03]\n",
      "Predicted Value 1181.0\n"
     ]
    }
   ],
   "source": [
    "my_weights = np.array([1.0, 1.0]) # the example weights, just a simple list, 1D\n",
    "my_features = example_features[0, ] # we'll use the first data point, 1st row 0, n all cols\n",
    "print \"Model Coefficients or weights\", my_weights\n",
    "print \"features matrix\", my_features\n",
    "predicted_value = np.dot(my_features, my_weights)\n",
    "print \"Predicted Value\", predicted_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "np.dot() also works when dealing with a matrix and a vector. Recall that the predictions from all the observations is just the RIGHT multiplication of the \"feature matrix\" by the \"weight vector\" (as in weights on the right) -- dot product between the features *matrix* and the weights *vector*. With this in mind finish the following predict_output function to compute the predictions for an entire matrix of features given the matrix and the weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_output(feature_matrix, weights):\n",
    "    # assume feature_matrix is a numpy matrix containing the features as columns \n",
    "    # and weights is a corresponding numpy array (our list of model coefficients)\n",
    "    # create the predictions vector by using np.dot()\n",
    "    \n",
    "    predictions = np.dot(feature_matrix, weights)\n",
    "    \n",
    "    return(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to test your code run the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1181.0\n",
      "2571.0\n"
     ]
    }
   ],
   "source": [
    "test_predictions = predict_output(example_features, my_weights)\n",
    "print test_predictions[0] # should be 1181.0\n",
    "print test_predictions[1] # should be 2571.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Computing the Derivative\n",
    "If we have the values of a single input feature in an array ‘feature’ and the prediction ‘errors’ (predictions - output) then the derivative of the regression cost function with respect to the weight of ‘feature’ is just twice the dot product between ‘feature’ and ‘errors’. Write a function that accepts a ‘feature’ array and ‘error’ array and returns the ‘derivative’ (a single number). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to move to computing the derivative of the regression cost function. Recall that the cost function is the sum over the data points of the squared difference between an observed output and a predicted output.\n",
    "\n",
    "Since the derivative of a sum is the sum of the derivatives we can compute the derivative for a single data point and then sum over data points. We can write the squared difference between the observed output and predicted output for a single point as follows:\n",
    "\n",
    "(w[0]\\*[CONSTANT] + w[1]\\*[feature_1] + ... + w[i] \\*[feature_i] + ... +  w[k]\\*[feature_k]   -   output)^2\n",
    "\n",
    "Where we have k features and a constant. So the derivative with respect to weight w[i] by the chain rule is:\n",
    "\n",
    "2\\*(w[0]\\*[CONSTANT] + w[1]\\*[feature_1] + ... + w[i] \\*[feature_i] + ... +  w[k]\\*[feature_k]   -   output)\\* [feature_i]\n",
    "\n",
    "The term inside the paranethesis is just the error (difference between prediction and output). So we can re-write this as:\n",
    "\n",
    "2\\*error\\*[feature_i]\n",
    "\n",
    "That is, the derivative for the weight for feature i is the sum (over data points) of 2 times the product of the error and the feature itself. In the case of the constant then this is just twice the sum of the errors!\n",
    "\n",
    "Recall that twice the sum of the product of two vectors is just twice the dot product of the two vectors. Therefore the derivative for the weight for feature_i is just two times the dot product between the values of feature_i and the current errors. \n",
    "\n",
    "With this in mind complete the following derivative function which computes the derivative of the weight given the value of the feature (over all data points) and the errors (over all data points)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feature_derivative(errors, feature):\n",
    "    # Assume that errors and feature are both numpy arrays of the same length (number of data points)\n",
    "    # compute twice the dot product of these vectors as 'derivative' and return the value\n",
    "    \n",
    "    derivative = 2 * np.dot(errors,feature)\n",
    "    return(derivative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test your feature derivartive run the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_numpy_data(): print data_sframe = features_sframe(head 5)\n",
      "+----------+-------------+\n",
      "| constant | sqft_living |\n",
      "+----------+-------------+\n",
      "|    1     |    1180.0   |\n",
      "|    1     |    2570.0   |\n",
      "|    1     |    770.0    |\n",
      "|    1     |    1960.0   |\n",
      "|    1     |    1680.0   |\n",
      "+----------+-------------+\n",
      "[5 rows x 2 columns]\n",
      " \n",
      "\n",
      "get_numpy_data(): print data_sframe converted to feature_matrix[0:5, :]\n",
      "[[  1.00000000e+00   1.18000000e+03]\n",
      " [  1.00000000e+00   2.57000000e+03]\n",
      " [  1.00000000e+00   7.70000000e+02]\n",
      " [  1.00000000e+00   1.96000000e+03]\n",
      " [  1.00000000e+00   1.68000000e+03]] \n",
      "\n",
      "-23345850016.0\n",
      "-23345850016.0\n"
     ]
    }
   ],
   "source": [
    "(example_features, example_output) = get_numpy_data(sales,     ['sqft_living'], 'price') \n",
    "# parameters in called function becomes   get_numpy_data(data_sframe, features,      output)\n",
    "\n",
    "my_weights = np.array([0.0, 0.0]) # this makes all the predictions 0 as B0=B1=0.0\n",
    "test_predictions = predict_output(example_features, my_weights) \n",
    "\n",
    "# just like SFrames, two numpy arrays can be elementwise subtracted with '-': \n",
    "errors = test_predictions - example_output \n",
    "# prediction errors in this case is just the -example_output, since subtracting from 0\n",
    "\n",
    "# let's compute the derivative with respect to 'constant', the \":\" indicates \"all rows\"\n",
    "feature = example_features[: , 0]    #all rows and 1st column\n",
    "derivative = feature_derivative(errors, feature)\n",
    "\n",
    "print derivative\n",
    "print -np.sum(example_output)*2 # should be the same as derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Gradient Descent\n",
    "Now we will use our predict_output and feature_derivative to write a gradient descent function. Although we can compute the derivative for all the features simultaneously (the gradient) we will explicitly loop over the features individually for simplicity. Write a gradient descent function that does the following:\n",
    "\n",
    "*    Accepts a numpy feature_matrix 2D array, a 1D output array, an array of initial weights, a step size and a convergence tolerance.\n",
    "*    While not converged updates each feature weight by subtracting the step size times the derivative for that feature given the current weights\n",
    "*    At each step computes the magnitude/length of the gradient (square root of the sum of squared components)\n",
    "*    When the magnitude of the gradient is smaller than the input tolerance returns the final weight vector.\n",
    "\n",
    "e.g. if you’re using SFrames and numpy you can complete the following function\n",
    "*regression_gradient_descent()*  - see below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will write a function that performs a gradient descent. The basic premise is simple. Given a starting point we update the current weights by moving in the negative gradient direction. Recall that the gradient is the direction of *increase* and therefore the negative gradient is the direction of *decrease* and we're trying to *minimize* a cost function. \n",
    "\n",
    "The amount by which we move in the negative gradient *direction*  is called the 'step size'. We stop when we are 'sufficiently close' to the optimum. We define this by requiring that the magnitude (length) of the gradient vector to be smaller than a fixed 'tolerance'.\n",
    "\n",
    "With this in mind, complete the following gradient descent function below using your derivative function above. For each step in the gradient descent we update the weight for each feature befofe computing our stopping criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import sqrt # recall that the magnitude/length of a vector [g[0], g[1], g[2]] is sqrt(g[0]^2 + g[1]^2 + g[2]^2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def regression_gradient_descent(feature_matrix, output, initial_weights, step_size, tolerance):\n",
    "    converged = False \n",
    "    weights = np.array(initial_weights) # make sure it's a numpy array\n",
    "    \n",
    "    while not converged:\n",
    "        # compute the predictions based on feature_matrix and weights using your predict_output() function\n",
    "        # print \"\\nWhile: Model Coefficients or weights\", weights\n",
    "        predictions = predict_output(feature_matrix, weights)\n",
    "        # predictions = np.dot(feature_matrix, weights)   #or simple do this\n",
    "        # compute the errors\n",
    "        errors = predictions - output\n",
    "\n",
    "        gradient_sum_squares = 0 # initialize the gradient sum of squares\n",
    "        # while we haven't reached the tolerance yet, update each feature's weight\n",
    "        for i in range(len(weights)): # loop over each weight\n",
    "            # Recall that feature_matrix[:, i] is the feature column associated with weights[i]\n",
    "            # compute the derivative for weight[i]:\n",
    "            feature = feature_matrix[: ,i] \n",
    "            derivative = feature_derivative(errors, feature)\n",
    "\n",
    "            # add the squared value of the derivative to the gradient sum of squares (for assessing convergence)\n",
    "            gradient_sum_squares += derivative**2\n",
    "            # print \"\\t for: gradient_sum_squares = \", gradient_sum_squares\n",
    "            # subtract the step size times the derivative from the current weight\n",
    "            weights -= (step_size*derivative)\n",
    "            print \"\\t derivative = \", derivative  \n",
    "            print \"\\t Weights = \", weights\n",
    "  \n",
    "        # compute the square-root of the gradient sum of squares to get the gradient matnigude:\n",
    "        gradient_magnitude = sqrt(gradient_sum_squares)\n",
    "        # print \"While: gradient_magnitude =\", gradient_magnitude, \"Tolerance=\",tolerance\n",
    "        if gradient_magnitude < tolerance:\n",
    "            converged = True\n",
    "            \n",
    "    return(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few things to note before we run the gradient descent. Since the gradient is a sum over all the data points and involves a product of an error and a feature the gradient itself will be very large since the features are large (squarefeet) and the output is large (prices). So while you might expect \"tolerance\" to be small, small is only relative to the size of the features. \n",
    "\n",
    "For similar reasons the step size will be much smaller than you might expect but this is because the gradient has such large values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Running the Gradient Descent as Simple Regression\n",
    "Now split the sales data into training and test data. Like previous notebooks it’s important to use the same seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data,test_data = sales.random_split(.8,seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For those students not using SFrames please download the training and testing data csv files!!\n",
    "\n",
    "# 8. Run the regression_gradient_descent function.\n",
    "In particular we will use the gradient descent to estimate the model from Week 1 using just an intercept and slope. Use the following parameters:\n",
    "\n",
    "*    features: ‘sqft_living’\n",
    "*    output: ‘price’\n",
    "*    initial weights: -47000, 1 (intercept, sqft_living respectively)\n",
    "*    step_size = 7e-12\n",
    "*    tolerance = 2.5e7\n",
    "\n",
    "Although the gradient descent is designed for multiple regression since the constant is now a feature we can use the gradient descent function to estimat the parameters in the simple regression on squarefeet. The folowing cell sets up the feature_matrix, output, initial weights and step size for the first model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_numpy_data(): print data_sframe = features_sframe(head 5)\n",
      "+----------+-------------+\n",
      "| constant | sqft_living |\n",
      "+----------+-------------+\n",
      "|    1     |    1180.0   |\n",
      "|    1     |    2570.0   |\n",
      "|    1     |    770.0    |\n",
      "|    1     |    1960.0   |\n",
      "|    1     |    1680.0   |\n",
      "+----------+-------------+\n",
      "[5 rows x 2 columns]\n",
      " \n",
      "\n",
      "get_numpy_data(): print data_sframe converted to feature_matrix[0:5, :]\n",
      "[[  1.00000000e+00   1.18000000e+03]\n",
      " [  1.00000000e+00   2.57000000e+03]\n",
      " [  1.00000000e+00   7.70000000e+02]\n",
      " [  1.00000000e+00   1.96000000e+03]\n",
      " [  1.00000000e+00   1.68000000e+03]] \n",
      "\n",
      "simple feature matrix = \n",
      "[[  1.00000000e+00   1.18000000e+03]\n",
      " [  1.00000000e+00   2.57000000e+03]\n",
      " [  1.00000000e+00   7.70000000e+02]\n",
      " [  1.00000000e+00   1.96000000e+03]\n",
      " [  1.00000000e+00   1.68000000e+03]]\n",
      "\n",
      "output = \n",
      "[ 221900.  538000.  180000.  604000.  510000.]\n"
     ]
    }
   ],
   "source": [
    "# let's test out the gradient descent\n",
    "simple_features = ['sqft_living']\n",
    "simple_output = 'price'\n",
    "\n",
    "# build a 2D matrix, 1st column constant 1s', 2nd col sqft_living, n a 1D target/response array\n",
    "(simple_feature_matrix, output) = get_numpy_data(train_data, simple_features, simple_output)\n",
    "initial_weights = np.array([-47000., 1.])\n",
    "step_size = 7e-12\n",
    "#step_size = 0.5\n",
    "tolerance = 2.5e9\n",
    "#tolerance = 1.0e2\n",
    "\n",
    "print \"simple feature matrix = \\n\", simple_feature_matrix[0:5 , :]\n",
    "print \"\\noutput = \\n\", output[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next run your gradient descent with the above parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t derivative =  -20314476454.0\n",
      "\t Weights =  [ -4.69998578e+04   1.14220134e+00]\n",
      "\t derivative =  -5.05515267032e+13\n",
      "\t Weights =  [-46645.99711174    355.00288826]\n",
      "\t derivative =  5298777356.79\n",
      "\t Weights =  [-46646.03420318    354.96579682]\n",
      "\t derivative =  1.31786304558e+13\n",
      "\t Weights =  [-46738.28461637    262.71538363]\n",
      "\t derivative =  -1378522061.16\n",
      "\t Weights =  [-46738.27496672    262.72503328]\n",
      "\t derivative =  -3.43563278644e+12\n",
      "\t Weights =  [-46714.22553722    286.77446278]\n",
      "\t derivative =  362230123.091\n",
      "\t Weights =  [-46714.22807283    286.77192717]\n",
      "\t derivative =  895656439713.0\n",
      "\t Weights =  [-46720.4976679    280.5023321]\n",
      "\t derivative =  -91578816.3181\n",
      "\t Weights =  [-46720.49702685    280.50297315]\n",
      "\t derivative =  -233497834960.0\n",
      "\t Weights =  [-46718.86254201    282.13745799]\n",
      "\t derivative =  26727830.286\n",
      "\t Weights =  [-46718.8627291    282.1372709]\n",
      "\t derivative =  60869340594.6\n",
      "\t Weights =  [-46719.28881449    281.71118551]\n",
      "\t derivative =  -4114362.13299\n",
      "\t Weights =  [-46719.28878569    281.71121431]\n",
      "\t derivative =  -15871310340.0\n",
      "\t Weights =  [-46719.17768651    281.82231349]\n",
      "\t derivative =  3926106.09525\n",
      "\t Weights =  [-46719.177714    281.822286]\n",
      "\t derivative =  4134750100.61\n",
      "\t Weights =  [-46719.20665725    281.79334275]\n",
      "\t derivative =  1829979.83901\n",
      "\t Weights =  [-46719.20667006    281.79332994]\n",
      "\t derivative =  -1080770610.74\n",
      "\t Weights =  [-46719.19910466    281.80089534]\n"
     ]
    }
   ],
   "source": [
    "simple_weights = regression_gradient_descent(simple_feature_matrix, output, initial_weights, step_size, tolerance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do your weights compare to those achieved in week 1 (don't expect them to be exactly the same)? \n",
    "\n",
    "**Quiz Question: What is the value of the weight for sqft_living -- the second element of ‘simple_weights’ (rounded to 1 decimal place)?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-46719.19910466    281.80089534]\n"
     ]
    }
   ],
   "source": [
    "print simple_weights  #slope term was 282.6 earlier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use your newly estimated weights and your predict_output() function to compute the predictions on all the TEST data (you will need to create a numpy array of the test feature_matrix and test output first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_numpy_data(): print data_sframe = features_sframe(head 5)\n",
      "+----------+-------------+\n",
      "| constant | sqft_living |\n",
      "+----------+-------------+\n",
      "|    1     |    1430.0   |\n",
      "|    1     |    2950.0   |\n",
      "|    1     |    1710.0   |\n",
      "|    1     |    2320.0   |\n",
      "|    1     |    1090.0   |\n",
      "+----------+-------------+\n",
      "[5 rows x 2 columns]\n",
      " \n",
      "\n",
      "get_numpy_data(): print data_sframe converted to feature_matrix[0:5, :]\n",
      "[[  1.00000000e+00   1.43000000e+03]\n",
      " [  1.00000000e+00   2.95000000e+03]\n",
      " [  1.00000000e+00   1.71000000e+03]\n",
      " [  1.00000000e+00   2.32000000e+03]\n",
      " [  1.00000000e+00   1.09000000e+03]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "(test_simple_feature_matrix, test_output) = get_numpy_data(test_data, simple_features, simple_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now compute your predictions using test_simple_feature_matrix and your weights from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "simple_predictions = predict_output(test_simple_feature_matrix,  simple_weights) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz Question: What is the predicted price for the 1st house in the TEST data set for model 1 (round to nearest dollar)?**\n",
    "\\$356256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 356256.08122763  784593.44214027  435160.33192206  607058.87807779\n",
      "  260443.77681296]\n"
     ]
    }
   ],
   "source": [
    "print simple_predictions[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[310000.0, 650000.0, 233000.0, 580500.0, 535000.0]\n"
     ]
    }
   ],
   "source": [
    "print test_data['price'][0:5]  #print test_data$price[0:5] SyntaxError: invalid syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 310000.,  650000.,  233000.,  580500.,  535000.])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(test_data['price'])[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have the predictions on test data, compute the RSS on the test data set. Save this value for comparison later. Recall that RSS is the sum of the squared errors (difference between prediction and output)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.75393109803e+14\n"
     ]
    }
   ],
   "source": [
    "# then compute the residuals (since we are squaring it doesn't matter which order you subtract)\n",
    "# residuals = test_data['price'] - predictions  -- this works but gives v.long RSS \n",
    "# fix convert SFrame column to array or list 1st\n",
    "residuals1 = np.array(test_data['price'])  - simple_predictions  \n",
    "# square the residuals and add them up\n",
    "RSS1 = (residuals1*residuals1).sum()\n",
    "print RSS1\n",
    "# This is what we got earlier using graphlabs regression function \n",
    "# 1.2012677033e+15\n",
    "\n",
    "#So appears gradient descent has lower RSS i.e better fit!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running a multiple regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will use more than one actual feature. Use the following code to produce the weights for a second model with the following parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_numpy_data(): print data_sframe = features_sframe(head 5)\n",
      "+----------+-------------+---------------+\n",
      "| constant | sqft_living | sqft_living15 |\n",
      "+----------+-------------+---------------+\n",
      "|    1     |    1180.0   |     1340.0    |\n",
      "|    1     |    2570.0   |     1690.0    |\n",
      "|    1     |    770.0    |     2720.0    |\n",
      "|    1     |    1960.0   |     1360.0    |\n",
      "|    1     |    1680.0   |     1800.0    |\n",
      "+----------+-------------+---------------+\n",
      "[5 rows x 3 columns]\n",
      " \n",
      "\n",
      "get_numpy_data(): print data_sframe converted to feature_matrix[0:5, :]\n",
      "[[  1.00000000e+00   1.18000000e+03   1.34000000e+03]\n",
      " [  1.00000000e+00   2.57000000e+03   1.69000000e+03]\n",
      " [  1.00000000e+00   7.70000000e+02   2.72000000e+03]\n",
      " [  1.00000000e+00   1.96000000e+03   1.36000000e+03]\n",
      " [  1.00000000e+00   1.68000000e+03   1.80000000e+03]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_features = ['sqft_living', 'sqft_living15'] \n",
    "# sqft_living15 is the average squarefeet for the nearest 15 neighbors. \n",
    "my_output = 'price'\n",
    "(multi_feature_matrix, multi_output) = get_numpy_data(train_data, model_features, my_output)\n",
    "multi_initial_weights = np.array([-100000., 1., 1.])\n",
    "step_size = 1e-12\n",
    "tolerance = 1e12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the above parameters to estimate the model weights. Record these values for your quiz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t derivative =  -22088131380.0\n",
      "\t Weights =  [ -9.99999779e+04   1.02208813e+00   1.02208813e+00]\n",
      "\t derivative =  -5.42241456419e+13\n",
      "\t Weights =  [ -9.99457538e+04   5.52462338e+01   5.52462338e+01]\n",
      "\t derivative =  -4.8982259336e+13\n",
      "\t Weights =  [-99896.77150689    104.22849311    104.22849311]\n",
      "\t derivative =  -7491384202.99\n",
      "\t Weights =  [-99896.76401551    104.23598449    104.23598449]\n",
      "\t derivative =  -1.90968331561e+13\n",
      "\t Weights =  [-99877.66718235    123.33281765    123.33281765]\n",
      "\t derivative =  -1.65844989607e+13\n",
      "\t Weights =  [-99861.08268339    139.91731661    139.91731661]\n",
      "\t derivative =  -2444902190.15\n",
      "\t Weights =  [-99861.08023849    139.91976151    139.91976151]\n",
      "\t derivative =  -6.95239107806e+12\n",
      "\t Weights =  [-99854.12784741    146.87215259    146.87215259]\n",
      "\t derivative =  -5.38373532424e+12\n",
      "\t Weights =  [-99848.74411209    152.25588791    152.25588791]\n",
      "\t derivative =  -700199730.268\n",
      "\t Weights =  [-99848.74341189    152.25658811    152.25658811]\n",
      "\t derivative =  -2.75373587361e+12\n",
      "\t Weights =  [-99845.98967601    155.01032399    155.01032399]\n",
      "\t derivative =  -1.51133474513e+12\n",
      "\t Weights =  [-99844.47834127    156.52165873    156.52165873]\n",
      "\t derivative =  -97009891.1376\n",
      "\t Weights =  [-99844.47824426    156.52175574    156.52175574]\n",
      "\t derivative =  -1.30214959138e+12\n",
      "\t Weights =  [-99843.17609467    157.82390533    157.82390533]\n",
      "\t derivative =  -172543325917.0\n",
      "\t Weights =  [-99843.00355134    157.99644866    157.99644866]\n",
      "\t derivative =  111528808.381\n",
      "\t Weights =  [-99843.00366287    157.99633713    157.99633713]\n",
      "\t derivative =  -800297776411.0\n",
      "\t Weights =  [-99842.20336509    158.79663491    158.79663491]\n",
      "\t derivative =  290312318157.0\n",
      "\t Weights =  [-99842.49367741    158.50632259    158.50632259]\n"
     ]
    }
   ],
   "source": [
    "multi_weights = regression_gradient_descent(multi_feature_matrix, multi_output, multi_initial_weights, step_size, tolerance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use your newly estimated weights and the predict_output function to compute the predictions on the TEST data. Don't forget to create a numpy array for these features from the test set first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-99842.49367741    158.50632259    158.50632259]\n",
      "get_numpy_data(): print data_sframe = features_sframe(head 5)\n",
      "+----------+-------------+---------------+\n",
      "| constant | sqft_living | sqft_living15 |\n",
      "+----------+-------------+---------------+\n",
      "|    1     |    1430.0   |     1780.0    |\n",
      "|    1     |    2950.0   |     2140.0    |\n",
      "|    1     |    1710.0   |     1030.0    |\n",
      "|    1     |    2320.0   |     2580.0    |\n",
      "|    1     |    1090.0   |     1570.0    |\n",
      "+----------+-------------+---------------+\n",
      "[5 rows x 3 columns]\n",
      " \n",
      "\n",
      "get_numpy_data(): print data_sframe converted to feature_matrix[0:5, :]\n",
      "[[  1.00000000e+00   1.43000000e+03   1.78000000e+03]\n",
      " [  1.00000000e+00   2.95000000e+03   2.14000000e+03]\n",
      " [  1.00000000e+00   1.71000000e+03   1.03000000e+03]\n",
      " [  1.00000000e+00   2.32000000e+03   2.58000000e+03]\n",
      " [  1.00000000e+00   1.09000000e+03   1.57000000e+03]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print multi_weights\n",
    "\n",
    "(multi_feature_matrix, multi_output) = get_numpy_data(test_data, model_features, my_output)\n",
    "\n",
    "multi_predictions = predict_output(multi_feature_matrix,  multi_weights) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz Question: What is the predicted price for the 1st house in the TEST data set for model 2 (round to nearest dollar)?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "408962.801836\n"
     ]
    }
   ],
   "source": [
    "print multi_predictions[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the actual price for the 1st house in the test data set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "310000.0\n"
     ]
    }
   ],
   "source": [
    "print test_data['price'][0]  #print test_data$price[0:5] SyntaxError: invalid syntax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz Question: Which estimate was closer to the true price for the 1st house on the Test data set, model 1 or model 2?**  Model 1 \\$356256, Model 2 had \\$408962.  So model 1, the simple model with only 1 predictor/feature 'sqft_living', had predictions closer to true price."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use your predictions and the output to compute the RSS for model 2 on TEST data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.77427930983e+14\n"
     ]
    }
   ],
   "source": [
    "# then compute the residuals (since we are squaring it doesn't matter which order you subtract)\n",
    "# residuals = test_data['price'] - predictions  -- this works but gives v.long RSS \n",
    "# fix convert SFrame column to array or list 1st\n",
    "residuals2 = np.array(test_data['price'])  - multi_predictions  \n",
    "# square the residuals and add them up\n",
    "RSS_2 = (residuals2*residuals2).sum()\n",
    "print RSS_2\n",
    "# This is what we got earlier using Model 1\n",
    "# 2.75393109803e+14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz Question: Which model (1 or 2) has lowest RSS on all of the TEST data? **\n",
    "Model 1 which only had feature 'sqft_living'. So appears extra feature 'sqft_living15' makes slighltly worse fit. So the more complex model had higher test RSS."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
